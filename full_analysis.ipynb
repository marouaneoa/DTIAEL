{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.rcParams['axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = pd.read_excel(\"pma.xlsx\")\n",
    "ts_data = ts_data.reset_index().drop(\"index\", axis=1)\n",
    "ts_data[\"time\"] = pd.to_datetime(ts_data[\"time\"])\n",
    "ts_data = ts_data.set_index(\"time\")\n",
    "ts_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = [1, 2, 3, 4]\n",
    "\n",
    "def map_season(month, day):\n",
    "    if (month == 12 and day >= 21) or (month == 1) or (month == 2) or (month == 3 and day < 21):\n",
    "        return seasons[0]  # Winter\n",
    "    elif (month == 3 and day >= 21) or (month == 4) or (month == 5) or (month == 6 and day < 21):\n",
    "        return seasons[1]  # Spring\n",
    "    elif (month == 6 and day >= 21) or (month == 7) or (month == 8) or (month == 9 and day < 21):\n",
    "        return seasons[2]  # Summer\n",
    "    else:\n",
    "        return seasons[3]  # Fall\n",
    "\n",
    "\n",
    "def is_special_day(date):\n",
    "    special_days = ['2016-07-06', '2017-06-25', '2016-09-12', '2017-09-01']\n",
    "    special_days = pd.to_datetime(special_days).date\n",
    "\n",
    "    return int(date in special_days)\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features based on the time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = df.index.date\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = ((df.index.dayofweek + 8) % 7) + 1\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    df[\"is_special_day\"] = df.apply(lambda row: is_special_day(row.date), axis=1)\n",
    "    df['season'] = df.apply(lambda row: map_season(row.month, row.dayofmonth), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_features(ts_data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for outliers, missing data and duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_values = df.isna().sum().sum()\n",
    "total_duplicates = df.duplicated().sum()\n",
    "\n",
    "print(f'total of missing values: {total_missing_values}')\n",
    "print(f'total of duplicated values: {total_duplicates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index, df[\"pma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over all time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try to see the development of the maximum power demand over time in 2016 and 2017 by months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group\n",
    "year_month_grouped_pma = df.groupby(['year', 'month']).pma.agg(years_monthly_pma='mean')\n",
    "\n",
    "title_style = {'family':'serif','color':'darkblue','size':18, 'weight':'bold'}\n",
    "labels_style = {'family':'serif','color':'black','size':15}\n",
    "def line_plot(data, x, y, title='', xlabel='', ylabel='', rotate_x=0):\n",
    "    plt.figure(figsize=[12, 6])\n",
    "    plt.title(title, fontdict=title_style)\n",
    "    plt.ylabel(ylabel, fontdict=labels_style)\n",
    "    plt.xlabel(xlabel, fontdict=labels_style)\n",
    "    plt.xticks(rotation=rotate_x)\n",
    "    sns.lineplot(data=data, x=x, y=y)\n",
    "    plt.show()\n",
    "\n",
    "# adding a column for the combination year-month\n",
    "year_month_grouped_pma['year_month'] = year_month_grouped_pma.index.map(lambda x: f'{x[0]}' + '-' + f'{x[1]}')\n",
    "# year_month_grouped_pma\n",
    "\n",
    "# plotting the data\n",
    "line_plot(year_month_grouped_pma, 'year_month', 'years_monthly_pma', 'monthly pma between 2016 and 2017', 'year-month', 'pma', 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice, for now, maximum power demand increases mostly during summer from May till October.  \n",
    "To see this better, let us take the average pma between the two years for ech month:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_grouped_pma = year_month_grouped_pma.groupby('month').years_monthly_pma.agg(average_monthly_pma='mean')\n",
    "# monthly_grouped_pma\n",
    "line_plot(monthly_grouped_pma, 'month', 'average_monthly_pma', 'Average Monthly pma', 'months', 'pma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='month', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, It is clear enough now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us find the hours of the day with most power demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_grouped_pma = df.groupby('hour').pma.agg(hourly_pma='mean')\n",
    "#hourly_grouped_pma\n",
    "line_plot(hourly_grouped_pma, 'hour', 'hourly_pma', 'Average Hourly PMA During One Day', 'Hour', 'pma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='hour', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>TODO</mark>\n",
    "- Add analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this one is for both two years, let us do it for each season and try to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasoned_hourly_grouped_pma = df.groupby(['season', 'hour']).pma.agg(seasoned_hourly_pma='mean')\n",
    "fall_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Fall']\n",
    "winter_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Winter']\n",
    "spring_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Spring']\n",
    "summer_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Summer']\n",
    "\n",
    "plt.figure(figsize=[14, 6])\n",
    "plt.title('Average Hourly PMA During Each Season')\n",
    "plt.ylabel('seasoned pma')\n",
    "plt.xlabel('hour')\n",
    "\n",
    "season_num_to_name = {\n",
    "    1: \"Winter\",\n",
    "    2: \"Spring\",\n",
    "    3: \"Summer\",\n",
    "    4: \"Fall\"\n",
    "}\n",
    "\n",
    "for season in seasons:\n",
    "    seasoned_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')==season]\n",
    "    #line_plot(seasoned_hourly, 'hour', 'seasoned_hourly_pma', f'Average Hourly Demand in {season}', 'Hour', 'pma')\n",
    "    sns.lineplot(data=seasoned_hourly, x='hour', y='seasoned_hourly_pma', label=season_num_to_name[season])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='season', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA in Week Days\n",
    "\n",
    "We'll now discover which day in the week has the highest consumption of energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the average usage of each day of the week\n",
    "dayName_grouped_pma = df.groupby('dayofweek').pma.agg(average_pma='mean')\n",
    "\n",
    "# We sort the days of the week\n",
    "dayName_grouped_pma = dayName_grouped_pma.reindex(range(8))\n",
    "\n",
    "# We plot the data\n",
    "line_plot(dayName_grouped_pma, 'dayofweek', 'average_pma', 'Average pma During Each Day of the Week', 'Day of the Week', 'pma')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We notice that the consumption is low in the weekends because most of people rest or go for picnics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA in special days (Eids Days)\n",
    "\n",
    "Let us discover how was the usage of people in \"Eids\", we might find some high usage due to family gathering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first define the special days (Eids in 2016 and 2017)\n",
    "special_days_df = df.loc[df[\"is_special_day\"] == 1]\n",
    "special_days_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the usage in Eid days to the overall usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot a comparison between the average hourly pma in the special days and the average hourly pma in the normal days\n",
    "normal_days_df = df[df[\"is_special_day\"] == 0]\n",
    "normal_days_hourly_grouped_pma = normal_days_df.groupby('hour').pma.agg(normal_hourly_pma='mean')\n",
    "special_days_hourly_grouped_pma = special_days_df.groupby('hour').pma.agg(special_hourly_pma='mean')\n",
    "# we plot the comparison\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.title('Average Hourly PMA During Special Days and Normal Days', fontdict=title_style)\n",
    "plt.ylabel('pma', fontdict=labels_style)\n",
    "plt.xlabel('hour', fontdict=labels_style)\n",
    "sns.lineplot(data=normal_days_hourly_grouped_pma, x='hour', y='normal_hourly_pma', label='normal days')\n",
    "sns.lineplot(data=special_days_hourly_grouped_pma, x='hour', y='special_hourly_pma', label='special days')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(data=df, x='is_special_day', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's remarkable that the usage in Eids is higher than in the normal days due to family gathering and especially that Eids were in the summer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Power demand reaches its maximum values during summer. Which logical, most people are in holidays thus staying at home most of the time compared to the rest of the year.\n",
    "* During one day, PMA reaches its climax at around 8pm.\n",
    "* During Winter, max demand is at its peak before 8pm, and after that time it starts decreasing. This could be due to many reasons. One of them is that people tend to sleep earlier at winter. Meanwhile during summer, it reaches its climax after 8pm and higher values as well 1pm and 4pm. Because, most people are at their homes with their AC (Air Conditioner) on at those times due to high temperatures outside.\n",
    "* Demand is low in all seasons during night (most logically) and medium during day where everyone are doing their activities and daily tasks.\n",
    "* Demand is low in weekends which is also logic because people are resting and companies are off\n",
    "* Demand is higher in Eids because of family gathering which cause a high consumption of electricity (lights on everywhere, hair dryer for women..etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping by days\n",
    "- Our data has each hour in a row, so we need to take the median of each 24 hours to represent that day\n",
    "- We choose the median to avoid outliers' effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df.groupby(\"date\").agg({\"pma\": \"median\", \"tmp\": \"median\", \"year\": \"min\", \"month\": \"min\", \"dayofmonth\": \"min\", \"dayofyear\": \"min\", \"is_special_day\": \"min\", \"season\": \"min\", \"dayofweek\": \"min\", \"quarter\": \"min\"})\n",
    "\n",
    "df_daily.index = pd.to_datetime(df_daily.index)\n",
    "\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cyclic attribute representation\n",
    "Since our data is a time series, we have many attributes that are cyclic, for example: month, which starts with 0 and ends with 11. We all know that month 11 (December) and month 1 (January) are right next to each other, but with this representation 11 is way bigger than 1, so if we apply any clustering or regression algorithm it will think that they are very appart from each other and fail to determine the real patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_transormation(df, columns: list, remove_original=False):\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        max_col_val = df[col].max()\n",
    "        df[f\"{col}_sin\"] = np.sin((2*np.pi*df[col]) / max_col_val)\n",
    "        df[f\"{col}_cos\"] = np.cos((2*np.pi*df[col]) / max_col_val)\n",
    "\n",
    "    if remove_original:\n",
    "        df = df.drop(columns=columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclical_columns = [\"year\", \"month\", \"dayofmonth\", \"dayofyear\", \"season\", \"dayofweek\", \"quarter\"]\n",
    "df_cyclic = cyclical_transormation(df_daily, columns=cyclical_columns, remove_original=True)\n",
    "df_cyclic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMA Regression to find most important features in order to justify the assumptions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, day_threshold=4):\n",
    "    train = df.loc[(df.reset_index().index + 1) % day_threshold != 0]\n",
    "    test = df.loc[(df.reset_index().index + 1) % day_threshold == 0]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cols = df_cyclic.columns\n",
    "TARGET = 'pma'\n",
    "FEATURES = cols[cols != TARGET]\n",
    "\n",
    "train, test = train_test_split(df_cyclic)\n",
    "\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "\n",
    "X_test = test[FEATURES]\n",
    "y_test = test[TARGET]\n",
    "\n",
    "# reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n",
    "#                        n_estimators=5000,\n",
    "#                        early_stopping_rounds=50,\n",
    "#                        objective='reg:linear',\n",
    "#                        max_depth=5,\n",
    "#                        learning_rate=0.01)\n",
    "\n",
    "gsc = RandomizedSearchCV(\n",
    "            estimator=xgb.XGBRegressor(),\n",
    "            param_distributions={\n",
    "                        \"learning_rate\": [0.005, 0.01, 0.02, 0.05, 0.1],\n",
    "                        \"max_depth\": [ 3, 4, 5, 6, 8],\n",
    "                        \"min_child_weight\": [ 1, 3, 5, 7],\n",
    "                        \"n_estimators\": [2000, 3000, 5000, 10000],\n",
    "                        \"colsample_bytree\":[ 0.3, 0.4],\n",
    "                        },\n",
    "            cv=3, scoring='neg_mean_squared_error', verbose=100, n_jobs=-1)\n",
    "\n",
    "grid_result = gsc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n All results:')\n",
    "print(grid_result.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(grid_result.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = xgb.XGBRegressor(\n",
    "    n_estimators=grid_result.best_params_[\"n_estimators\"],\n",
    "    max_depth=grid_result.best_params_[\"max_depth\"],\n",
    "    colsample_bytree=grid_result.best_params_[\"colsample_bytree\"],\n",
    "    min_child_weight=grid_result.best_params_[\"min_child_weight\"],\n",
    "    learning_rate=grid_result.best_params_[\"learning_rate\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_pd = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=['importance'])\n",
    "feature_importance_pd.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key Notes\n",
    "- we see that Quarter, Season and temperature are one of the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['prediction'] = reg.predict(X_test)\n",
    "df_pred = df_daily.merge(test[['prediction']], how='left', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(df_pred[\"pma\"])\n",
    "plt.plot(test[\"prediction\"])\n",
    "plt.legend(['Truth Data', 'Predictions'])\n",
    "plt.title('Raw Data and Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_score = mean_squared_error(test['pma'], test['prediction'])\n",
    "rmse_score = np.sqrt(mse_score)\n",
    "\n",
    "print(f'MSE Score on Test set: {mse_score:0.2f}')\n",
    "print(f'RMSE Score on Test set: {rmse_score:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key notes\n",
    "- our regression model succeeds quiet well on predicting the values of the test data\n",
    "- Using our model we can predict the PMA values of future records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def pca_transform(df, explained_variance_threshold=0.95):\n",
    "    pca = PCA(n_components=explained_variance_threshold)\n",
    "    \n",
    "    return pca.fit_transform(df)\n",
    "\n",
    "def normalise(df):\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    return standard_scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_cyclic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we remove \"pma\" since we probably don't know it in the future\n",
    "# and so that our model doesn't depend on it in that case\n",
    "\n",
    "# also not sure if we really need to split data in clustering or not\n",
    "train_no_pma = train.drop(columns=[\"pma\"])\n",
    "test_no_pma = test.drop(columns=[\"pma\"])\n",
    "\n",
    "# when we use PCA, we get inconsistent clustering (example: putting december 31st and january 1st in diff clusters), so ig we won't use it\n",
    "reduced_train = normalise(pca_transform(train_no_pma, explained_variance_threshold=0.95))\n",
    "reduced_test = normalise(pca_transform(test_no_pma, explained_variance_threshold=0.95))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_plot_silhouette_score(df, max_k=11):\n",
    "    silhouette = []\n",
    "    k_value = range(2, 11) # silhouette needs at least 2 clusters\n",
    "\n",
    "    for k in k_value:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "        kmeans.fit(df)\n",
    "        cluster_labels = kmeans.labels_\n",
    "        silhouette.append(silhouette_score(df, cluster_labels))\n",
    "\n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.plot(k_value, silhouette, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette')\n",
    "    plt.title('Silhouette Metric To Find Optimal Number Of Clusters')\n",
    "    plt.show()\n",
    "\n",
    "    return k_value[np.array(silhouette).argmax(axis=0)]\n",
    "\n",
    "def perform_kmeans_clustering(df, k):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    cluster = kmeans.fit(df)\n",
    "\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, supposed_target=\"pma\", k_override=None):\n",
    "    no_target_df = df.drop(columns=[supposed_target])\n",
    "    k_value = k_means_plot_silhouette_score(no_target_df) if k_override is None else k_override\n",
    "    cluster = perform_kmeans_clustering(no_target_df, k=k_value)\n",
    "    colormap = np.array([\"r\", \"g\", \"b\"])\n",
    "    plt.scatter(df.index, df[\"pma\"], c=colormap[cluster.labels_])\n",
    "    print(f'SSE = {cluster.inertia_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(df_cyclic, k_override=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
