{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type identification of Algerian electricity load in Annaba city\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the domain of energy management and consumption, understanding the patterns and characteristics of electricity load is crucial for efficient resource utilization and future planning. This data mining project focuses on analyzing the Day Type Identification of electricity load in Annaba City, Algeria.\n",
    "\n",
    "Annaba, one of the rich cities in industrial and commercial landscapes in Algeria, it experiences diverse electricity consumption patterns influenced by factors such as industrial activities, residential needs, and commercial enterprises. Analyzing and categorizing these patterns into distinct day types can provide valuable insights for optimizing energy distribution, predicting peak loads, and enhancing overall energy sustainability.\n",
    "### Objective\n",
    "\n",
    "One of the challenges faced in energy management is the dynamic nature of electricity consumption, influenced by various factors. Unpredictable times of high demand, inefficient resource allocation, and the lack of a comprehensive understanding of consumption patterns, all this can lead to bad energy distribution over time. Our objective is to address these challenges by leveraging data mining techniques to identify distinct day types based on electricity load. By categorizing days into specific types such as weekdays, seasons, holidays, or special events, we aim to provide a nuanced understanding of consumption dynamics. This, in turn, can contribute to the resolution of problems related to inefficient resource allocation, unexpected demand fluctuations, and the lack of a tailored approach to energy distribution.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We'll be working with the 'pma.csv' dataset, it contains in every hour how much electricity it is consumed and the temperature at that time, from the start of the year 2016 to the end of 2017.\n",
    "\n",
    "### Project steps:\n",
    "\n",
    "- Feature Engineering.\n",
    "- Data cleaning\n",
    "- Exploratory data analysis\n",
    "- Data preprocessing\n",
    "- Modeling: Regression\n",
    "- Modeling: clustering\n",
    "\n",
    "More detail about the steps and the code of it are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.rcParams['axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = pd.read_excel(\"pma.xlsx\")\n",
    "ts_data = ts_data.reset_index().drop(\"index\", axis=1)\n",
    "ts_data[\"time\"] = pd.to_datetime(ts_data[\"time\"])\n",
    "ts_data = ts_data.set_index(\"time\")\n",
    "ts_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Here we will devide the column of time into three parts: date and hour.\n",
    "\n",
    "Also, we will add more columns to our dataset, which are\n",
    "- season\n",
    "- is_special_day\n",
    "- dayofmonth\n",
    "- dayofyear\n",
    "- year\n",
    "- month\n",
    "- quarter\n",
    "- dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = [1, 2, 3, 4]\n",
    "\n",
    "def map_season(month, day):\n",
    "    if (month == 12 and day >= 21) or (month == 1) or (month == 2) or (month == 3 and day < 21):\n",
    "        return seasons[0]  # Winter\n",
    "    elif (month == 3 and day >= 21) or (month == 4) or (month == 5) or (month == 6 and day < 21):\n",
    "        return seasons[1]  # Spring\n",
    "    elif (month == 6 and day >= 21) or (month == 7) or (month == 8) or (month == 9 and day < 21):\n",
    "        return seasons[2]  # Summer\n",
    "    else:\n",
    "        return seasons[3]  # Fall\n",
    "\n",
    "\n",
    "def is_special_day(date):\n",
    "    special_days = ['2016-07-06', '2017-06-25', '2016-09-12', '2017-09-01']\n",
    "    special_days = pd.to_datetime(special_days).date\n",
    "\n",
    "    return int(date in special_days)\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features based on the time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = df.index.date\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = ((df.index.dayofweek + 8) % 7) + 1\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    df[\"is_special_day\"] = df.apply(lambda row: is_special_day(row.date), axis=1)\n",
    "    df['season'] = df.apply(lambda row: map_season(row.month, row.dayofmonth), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_features(ts_data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking missing data and duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_values = df.isna().sum().sum()\n",
    "total_duplicates = df.duplicated().sum()\n",
    "\n",
    "print(f'total of missing values: {total_missing_values}')\n",
    "print(f'total of duplicated values: {total_duplicates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General overview of the power demand over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index, df[\"pma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over all time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will see the development of the power demand over time in 2016 and 2017 by calculating the mean power demand on each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_grouped_pma = df.groupby(['year', 'month']).pma.agg(years_monthly_pma='mean')\n",
    "\n",
    "title_style = {'family':'serif','color':'darkblue','size':18, 'weight':'bold'}\n",
    "labels_style = {'family':'serif','color':'black','size':15}\n",
    "def line_plot(data, x, y, title='', xlabel='', ylabel='', rotate_x=0):\n",
    "    plt.figure(figsize=[12, 6])\n",
    "    plt.title(title, fontdict=title_style)\n",
    "    plt.ylabel(ylabel, fontdict=labels_style)\n",
    "    plt.xlabel(xlabel, fontdict=labels_style)\n",
    "    plt.xticks(rotation=rotate_x)\n",
    "    sns.lineplot(data=data, x=x, y=y)\n",
    "    plt.show()\n",
    "\n",
    "# adding a column for the combination year-month\n",
    "year_month_grouped_pma['year_month'] = year_month_grouped_pma.index.map(lambda x: f'{x[0]}' + '-' + f'{x[1]}')\n",
    "# year_month_grouped_pma\n",
    "\n",
    "# plotting the data\n",
    "line_plot(year_month_grouped_pma, 'year_month', 'years_monthly_pma', 'monthly pma between 2016 and 2017', 'year-month', 'pma', 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice earlier, the maximum power demand increases mostly during summer from May till October.  \n",
    "To see this better, let us take the average pma between the two years for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_grouped_pma = year_month_grouped_pma.groupby('month').years_monthly_pma.agg(average_monthly_pma='mean')\n",
    "# monthly_grouped_pma\n",
    "line_plot(monthly_grouped_pma, 'month', 'average_monthly_pma', 'Average Monthly pma', 'months', 'pma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='month', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let explore pma in days, as we did in months before, we calculate the mean power demand of each hour in the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_grouped_pma = df.groupby('hour').pma.agg(hourly_pma='mean')\n",
    "#hourly_grouped_pma\n",
    "line_plot(hourly_grouped_pma, 'hour', 'hourly_pma', 'Average Hourly PMA During One Day', 'Hour', 'pma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='hour', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can notice that the consumption of electricity starts decreasing in the morning till it achives its minimum. then it gets higher and higher till it reaches its maximum at night.\n",
    "\n",
    "However, this analysis is considering all the days in both two years, let us split the days according to the season it belongs to and try to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasoned_hourly_grouped_pma = df.groupby(['season', 'hour']).pma.agg(seasoned_hourly_pma='mean')\n",
    "fall_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Fall']\n",
    "winter_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Winter']\n",
    "spring_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Spring']\n",
    "summer_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Summer']\n",
    "\n",
    "plt.figure(figsize=[14, 6])\n",
    "plt.title('Average Hourly PMA During Each Season')\n",
    "plt.ylabel('seasoned pma')\n",
    "plt.xlabel('hour')\n",
    "\n",
    "season_num_to_name = {\n",
    "    1: \"Winter\",\n",
    "    2: \"Spring\",\n",
    "    3: \"Summer\",\n",
    "    4: \"Fall\"\n",
    "}\n",
    "\n",
    "for season in seasons:\n",
    "    seasoned_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')==season]\n",
    "    #line_plot(seasoned_hourly, 'hour', 'seasoned_hourly_pma', f'Average Hourly Demand in {season}', 'Hour', 'pma')\n",
    "    sns.lineplot(data=seasoned_hourly, x='hour', y='seasoned_hourly_pma', label=season_num_to_name[season])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, we notice that the consumption in summer is the highest, followed by fall and winter, then spring\n",
    "\n",
    "Another thing we can see here is unlike other seasons, in the summer, the consumption gets higher in the middle of the day (from around 1pm to 4pm)\n",
    "this can be due to the use of air conditioners, because of the high temperature at that time of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='season', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, the consumption in the summer is the highest, followed by winter and fall, then spring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA in Week Days\n",
    "\n",
    "We'll now discover which day in the week has the highest consumption of energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the average usage of each day of the week\n",
    "dayName_grouped_pma = df.groupby('dayofweek').pma.agg(average_pma='mean')\n",
    "\n",
    "# We sort the days of the week\n",
    "dayName_grouped_pma = dayName_grouped_pma.reindex(range(8))\n",
    "\n",
    "# We plot the data\n",
    "line_plot(dayName_grouped_pma, 'dayofweek', 'average_pma', 'Average pma During Each Day of the Week', 'Day of the Week', 'pma')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the consumption is lower in the weekends, this is probably due to reduced industrial and commercial activities, especially in Friday (day 6); we find almost all supermarkets, industries, schools, closed at that time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA in special days (Eid Days)\n",
    "\n",
    "Let us discover how was the usage of people in \"Eids\", we might find some high usage due to family gathering and social and cultural activities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first define the special days (Eids in 2016 and 2017)\n",
    "special_days_df = df.loc[df[\"is_special_day\"] == 1]\n",
    "special_days_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the usage in Eid days to the overall usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot a comparison between the average hourly pma in the special days and the average hourly pma in the normal days\n",
    "normal_days_df = df[df[\"is_special_day\"] == 0]\n",
    "normal_days_hourly_grouped_pma = normal_days_df.groupby('hour').pma.agg(normal_hourly_pma='mean')\n",
    "special_days_hourly_grouped_pma = special_days_df.groupby('hour').pma.agg(special_hourly_pma='mean')\n",
    "# we plot the comparison\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.title('Average Hourly PMA During Special Days and Normal Days', fontdict=title_style)\n",
    "plt.ylabel('pma', fontdict=labels_style)\n",
    "plt.xlabel('hour', fontdict=labels_style)\n",
    "sns.lineplot(data=normal_days_hourly_grouped_pma, x='hour', y='normal_hourly_pma', label='normal days')\n",
    "sns.lineplot(data=special_days_hourly_grouped_pma, x='hour', y='special_hourly_pma', label='special days')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(data=df, x='is_special_day', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's remarkable that the usage in Eids is higher than in the normal days due to the increased social Activities and family gathering .\n",
    "the consumption gets higher in the middle of the day (from around 1pm to 4pm) because most of Eids were in the summer in both years 2016 and 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMA in Ramadan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataframe containing the PMA of Ramadan days of 2016 and 2017 knowing that Ramadan days are from 6th of June to 5th of July in 2016 and from 26th of May to 24th of June in 2017\n",
    "ramadan_df_2016 = df.loc[(df.index.date >= pd.to_datetime('2016-06-06').date()) & (df.index.date <= pd.to_datetime('2016-07-05').date())]\n",
    "ramadan_df_2017 = df.loc[(df.index.date >= pd.to_datetime('2017-05-26').date()) & (df.index.date <= pd.to_datetime('2017-06-24').date())]\n",
    "ramadan_df = pd.concat([ramadan_df_2016, ramadan_df_2017])\n",
    "ramadan_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average hourly pma during Ramadan days and the average hourly pma during normal days\n",
    "normal_days_df = df.loc[(df.index.date < pd.to_datetime('2016-06-06').date()) | (df.index.date > pd.to_datetime('2016-07-05').date())]\n",
    "normal_days_df = normal_days_df.loc[(normal_days_df.index.date < pd.to_datetime('2017-05-26').date()) | (normal_days_df.index.date > pd.to_datetime('2017-06-24').date())]\n",
    "normal_days_hourly_grouped_pma = normal_days_df.groupby('hour').pma.agg(normal_hourly_pma='mean')\n",
    "ramadan_days_hourly_grouped_pma = ramadan_df.groupby('hour').pma.agg(ramadan_hourly_pma='mean')\n",
    "# we plot the comparison\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.title('Average Hourly PMA During Ramadan Days and Normal Days', fontdict=title_style)\n",
    "plt.ylabel('pma', fontdict=labels_style)\n",
    "plt.xlabel('hour', fontdict=labels_style)\n",
    "plt.xticks(rotation=0)\n",
    "plt.yticks(rotation=0)\n",
    "sns.lineplot(data=normal_days_hourly_grouped_pma, x='hour', y='normal_hourly_pma', label='normal days')\n",
    "sns.lineplot(data=ramadan_days_hourly_grouped_pma, x='hour', y='ramadan_hourly_pma', label='ramadan days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the consumption gets higher then usual in ramadan at night (from 8pm to 3pm), and it gets really low in the early morning (from 4am to 10am), this is due to many people not sleeping at the start of the night to go to tarawih, practice night prayer, or simply just waiting for sahour time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key notes for EDA step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Power demand reaches its maximum value during summer. Which is logical, the high temperature at that time leads to the higher use of air conditioners.\n",
    "* During the day, PMA reaches its max at around 8pm.\n",
    "* During Winter, max demand is at its peak before 8pm, and after that time it starts decreasing. This could be due to many reasons. One of them is that people tend to sleep earlier at winter. Meanwhile during summer, it reaches its max after 8pm and we notice higher consumption as well between 1pm and 4pm.\n",
    "* The demand increases in summer in the period from 1pm to 4pm, this can be due to the use of air conditioners, because of the high temperature at that time of the day.\n",
    "* Demand is low in all seasons during early morning and medium during the middle of the day where everyone are doing their activities and daily tasks.\n",
    "* Demand is low in weekends (especially in Fridays) due to reduced industrial and commercial activities.\n",
    "* Demand is higher in Eids because of social and cultural activities and family gathering which cause a high consumption of electricity.\n",
    "* The consumption at night is higher then usual in Ramadan due to the fact that people sleep late waiting for sahour, going to tarawih and practicing night pray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping by days\n",
    "- Our data has each hour in a row, so we need to take the median of each 24 hours to represent that day\n",
    "- We choose the median to avoid outliers' effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df.groupby(\"date\").agg({\"pma\": \"median\", \"tmp\": \"median\", \"year\": \"min\", \"month\": \"min\", \"dayofmonth\": \"min\", \"dayofyear\": \"min\", \"is_special_day\": \"min\", \"season\": \"min\", \"dayofweek\": \"min\", \"quarter\": \"min\"})\n",
    "\n",
    "df_daily.index = pd.to_datetime(df_daily.index)\n",
    "\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cyclic attribute representation\n",
    "Since our data is a time series, we have many attributes that are cyclic, for example: months, which starts with 0 and ends with 11. We all know that month 11 (December) and month 1 (January) are right next to each other, but with this representation 11 is way bigger than 1, so if we apply any clustering or regression algorithm it will think that those two values are very appart from each other, which is not the case in reality. This will lead to a fail to determine the real patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_transormation(df, columns: list, remove_original=False):\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        max_col_val = df[col].max()\n",
    "        df[f\"{col}_sin\"] = np.sin((2*np.pi*df[col]) / max_col_val)\n",
    "        df[f\"{col}_cos\"] = np.cos((2*np.pi*df[col]) / max_col_val)\n",
    "\n",
    "    if remove_original:\n",
    "        df = df.drop(columns=columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclical_columns = [\"year\", \"month\", \"dayofmonth\", \"dayofyear\", \"season\", \"dayofweek\", \"quarter\"]\n",
    "df_cyclic = cyclical_transormation(df_daily, columns=cyclical_columns, remove_original=True)\n",
    "df_cyclic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMA Regression to find most important features in order to justify the assumptions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the function \"train_test_split\" in order to split our data into two sets, one for training and one for testing, the way we are doing this is by taking the variable \"day_threshold\" into account, which allows us to customize the interval between testing set entries. This parameter ensures that the division of consecutive days between the training and testing sets reflects the temporal dependencies present in our time-series data. For example: if the day_threshold equal to 4, the testing data will be the day number 4, 8, 12, 16, 20 ... and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, day_threshold=4):\n",
    "    train = df.loc[(df.reset_index().index + 1) % day_threshold != 0]\n",
    "    test = df.loc[(df.reset_index().index + 1) % day_threshold == 0]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to find the best hyper parameters set that leads to the best training results, we do that by considering all possible compinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cols = df_cyclic.columns\n",
    "TARGET = 'pma'\n",
    "FEATURES = cols[cols != TARGET]\n",
    "\n",
    "train, test = train_test_split(df_cyclic)\n",
    "\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "\n",
    "X_test = test[FEATURES]\n",
    "y_test = test[TARGET]\n",
    "\n",
    "gsc = RandomizedSearchCV(\n",
    "            estimator=xgb.XGBRegressor(),\n",
    "            param_distributions={\n",
    "                        \"learning_rate\": [0.005, 0.01, 0.02, 0.05, 0.1],\n",
    "                        \"max_depth\": [ 3, 4, 5, 6, 8],\n",
    "                        \"min_child_weight\": [ 1, 3, 5, 7],\n",
    "                        \"n_estimators\": [2000, 3000, 5000, 10000],\n",
    "                        \"colsample_bytree\":[ 0.3, 0.4],\n",
    "                        },\n",
    "            cv=3, scoring='neg_mean_squared_error', verbose=100, n_jobs=-1)\n",
    "\n",
    "grid_result = gsc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n All results:')\n",
    "print(grid_result.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(grid_result.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our model with the chosing set of hyper parameters and we train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = xgb.XGBRegressor(\n",
    "    n_estimators=grid_result.best_params_[\"n_estimators\"],\n",
    "    max_depth=grid_result.best_params_[\"max_depth\"],\n",
    "    colsample_bytree=grid_result.best_params_[\"colsample_bytree\"],\n",
    "    min_child_weight=grid_result.best_params_[\"min_child_weight\"],\n",
    "    learning_rate=grid_result.best_params_[\"learning_rate\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We order the features according to the most important ones, meaning the ones that have a bigger impact on the electricity consumption are first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_pd = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=['importance'])\n",
    "feature_importance_pd.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key Notes\n",
    "- we see that Quarter, Season and temperature are one of the most important features that impacts pma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['prediction'] = reg.predict(X_test)\n",
    "df_pred = df_daily.merge(test[['prediction']], how='left', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(df_pred[\"pma\"])\n",
    "plt.plot(test[\"prediction\"])\n",
    "plt.legend(['Truth Data', 'Predictions'])\n",
    "plt.title('Raw Data and Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_score = mean_squared_error(test['pma'], test['prediction'])\n",
    "rmse_score = np.sqrt(mse_score)\n",
    "\n",
    "print(f'MSE Score on Test set: {mse_score:0.2f}')\n",
    "print(f'RMSE Score on Test set: {rmse_score:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key notes\n",
    "- our regression model succeeds quiet well on predicting the values of the test data\n",
    "- Using our model we can predict the PMA values of future records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make clustering to our data acording to all columns except the pma one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def pca_transform(df, explained_variance_threshold=0.95):\n",
    "    pca = PCA(n_components=explained_variance_threshold)\n",
    "    \n",
    "    return pca.fit_transform(df)\n",
    "\n",
    "def normalise(df):\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    return standard_scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_cyclic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we remove \"pma\" since we probably don't know it in the future\n",
    "# and so that our model doesn't depend on it in that case\n",
    "\n",
    "\n",
    "train_no_pma = train.drop(columns=[\"pma\"])\n",
    "test_no_pma = test.drop(columns=[\"pma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use silhouette score to find the best number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_plot_silhouette_score(df, max_k=11):\n",
    "    silhouette = []\n",
    "    k_value = range(2, 11) # silhouette needs at least 2 clusters\n",
    "\n",
    "    for k in k_value:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "        kmeans.fit(df)\n",
    "        cluster_labels = kmeans.labels_\n",
    "        silhouette.append(silhouette_score(df, cluster_labels))\n",
    "\n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.plot(k_value, silhouette, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette')\n",
    "    plt.title('Silhouette Metric To Find Optimal Number Of Clusters')\n",
    "    plt.show()\n",
    "\n",
    "    return k_value[np.array(silhouette).argmax(axis=0)]\n",
    "\n",
    "def perform_kmeans_clustering(df, k):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    cluster = kmeans.fit(df)\n",
    "\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, supposed_target=\"pma\", k_override=None):\n",
    "    no_target_df = df.drop(columns=[supposed_target])\n",
    "    k_value = k_means_plot_silhouette_score(no_target_df) if k_override is None else k_override\n",
    "    cluster = perform_kmeans_clustering(no_target_df, k=k_value)\n",
    "    colormap = np.array([\"r\", \"g\", \"b\"])\n",
    "    plt.scatter(df.index, df[\"pma\"], c=colormap[cluster.labels_])\n",
    "    print(f'SSE = {cluster.inertia_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(df_cyclic, k_override=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated three clusters:\n",
    "\n",
    "Red cluster (Low Usage):\n",
    "    Happens in Fall and Spring.\n",
    "    Makes sense because people don't use air conditioners or heaters much during these times.\n",
    "\n",
    "Purple Group (Medium Usage):\n",
    "    Mostly in winter.\n",
    "    Winter nights are longer, so lights stay on more, the higher consumption here might also be due to the use of electric heaters.\n",
    "\n",
    "Blue Group (High Usage):\n",
    "    Mostly in summer.\n",
    "    This is when people use air conditioners a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In our investigation into electricity consumption patterns in Annaba City, Algeria, we initiated a thorough exploration, beginning with feature engineering, data cleaning, and exploratory data analysis (EDA), then modeling with regression and clustering. Our primary goal was to determine specific day categories based on electricity load, aiming to uncover crucial insights for optimizing energy management and resource utilization.\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Seasonal Consumption Patterns:**\n",
    "   - We observed clear seasonal variations in electricity consumption, with higher demand during summer, attributable to the widespread use of air conditioners.\n",
    "   - Winter months showed medium consumption, influenced by factors such as the use of electric heaters and longer nights resulting in increased lighting usage.\n",
    "\n",
    "2. **Temporal Trends:**\n",
    "   - The analysis of power demand over time revealed daily and monthly trends. Consumption tends to peak around 8 pm, and the summer months experience the highest demand.\n",
    "\n",
    "3. **Special Events Impact:**\n",
    "   - Consumption during special events like Eids and Ramadan showcased distinct patterns, with higher usage during these times, likely due to increased social activities, family gatherings, and altered daily routines.\n",
    "\n",
    "4. **Weekday and Weekend Variation:**\n",
    "   - Weekdays exhibited higher energy consumption compared to weekends, emphasizing the impact of industrial and commercial activities on electricity demand.\n",
    "   - Fridays, in particular, showed a notable decrease in consumption, aligning with reduced business and school activities.\n",
    "\n",
    "**Modeling (with regression):**\n",
    "- We employed regression analysis to predict electricity consumption. Key features influencing consumption included the quarter, season, and temperature.\n",
    "- The model created could be used to predict electricity consumption in the future.\n",
    "\n",
    "**Modeling (with clustering):**\n",
    "- K-Means clustering identified three clusters:\n",
    "  - Red Cluster (Low Usage): Fall and Spring, associated with minimal use of air conditioners and heaters.\n",
    "  - Purple Cluster (Medium Usage): Winter, marked by longer nights and increased lighting usage, possibly influenced by electric heaters.\n",
    "  - Blue Cluster (High Usage): Summer, characterized by elevated demand due to extensive use of air conditioners.\n",
    "\n",
    "**Implications:**\n",
    "- Understanding these consumption patterns is crucial for optimizing energy distribution, predicting peak loads, and enhancing overall energy sustainability.\n",
    "- Our findings can guide efficient resource allocation and aid in developing tailored approaches to energy distribution based on specific day types.\n",
    "\n",
    "The insights derived from these analysis provide a valuable foundation for informed decision-making in energy management in Annaba or any similar Algerian City. One of the best dicisions we could make using these analysis is setting the price of electricity according to the expected amount of consumption at a specific time (if the consumption is high we make the price of the electricity higher), by doing this we will control the consumption during peak times and encourage people to use electricity in times when the consumption is low. This dynamic pricing strategy can lead to several benefits, such as preventing electricity cut-off because of high usage, electicity usage will be balanced throw out time, thus electricity destributing systems will have less problems in their work, and people that consider this dynamic pricing will save more money.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
