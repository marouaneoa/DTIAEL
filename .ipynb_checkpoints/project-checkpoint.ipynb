{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Type Identification of Algerian Electricity Load\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "This Jupyter notebook explores the identification of different day types based on electricity load patterns in an Algerian city. We will analyze a dataset containing hourly recordings of Maximum Power Demand (PMA) and Temperature for two years, from January 1st, 2016, to December 31st, 2017.\n",
    "\n",
    "**Data Source:**\n",
    "\n",
    "The dataset used in this analysis is stored in a file named `pma.xlsx`. It contains three columns:\n",
    "\n",
    "* `time`: Date and time (hourly)\n",
    "* `pma`: Maximum Power Demand (MW)\n",
    "* `tmp`: Temperature (Â°C)\n",
    "\n",
    "**Software and Tools:**\n",
    "\n",
    "This project will utilize Python libraries such as:\n",
    "\n",
    "* `pandas` for data manipulation and analysis\n",
    "* `numpy` for scientific computing\n",
    "* `matplotlib` and `seaborn` for data visualization\n",
    "* `scikit-learn` for machine learning and clustering algorithms\n",
    "\n",
    "**Let's begin by importing the necessary libraries and reading the data into a Pandas dataframe.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"pma.xlsx\"\n",
    "\n",
    "df = pd.read_excel(data_path, skiprows=1)\n",
    "\n",
    "df.columns = [\"time\", \"pma\", \"tmp\"]\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of rows: {df.shape[0]}')\n",
    "print(f'number of columns: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_values = df.isna().sum().sum()\n",
    "print(f'total of missing values: {total_missing_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duplicates = df.duplicated().sum()\n",
    "print(f'total of missing values: {total_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spread tme into several columns for better groupping\n",
    "df['hour'] = df.time.dt.hour\n",
    "df['day'] = df.time.dt.day\n",
    "df['month'] = df.time.dt.month\n",
    "df['year'] = df.time.dt.year\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over all time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try to see the development of the maximum power demand over time in 2016 and 2017 by months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group\n",
    "year_month_grouped_pma = df.groupby(['year', 'month']).pma.agg(years_monthly_pma='mean')\n",
    "\n",
    "title_style = {'family':'serif','color':'darkblue','size':18, 'weight':'bold'}\n",
    "labels_style = {'family':'serif','color':'black','size':15}\n",
    "def line_plot(data, x, y, title='', xlabel='', ylabel='', rotate_x=0):\n",
    "    plt.figure(figsize=[12, 6])\n",
    "    plt.title(title, fontdict=title_style)\n",
    "    plt.ylabel(ylabel, fontdict=labels_style)\n",
    "    plt.xlabel(xlabel, fontdict=labels_style)\n",
    "    plt.xticks(rotation=rotate_x)\n",
    "    sns.lineplot(data=data, x=x, y=y)\n",
    "    plt.show()\n",
    "\n",
    "# adding a column for the combination year-month\n",
    "year_month_grouped_pma['year_month'] = year_month_grouped_pma.index.map(lambda x: f'{x[0]}' + '-' + f'{x[1]}')\n",
    "# year_month_grouped_pma\n",
    "\n",
    "# plotting the data\n",
    "line_plot(year_month_grouped_pma, 'year_month', 'years_monthly_pma', 'monthly pma between 2016 and 2017', 'year-month', 'pma', 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice, for now, maximum power demand increases mostly during summer from May till October.  \n",
    "To see this better, let us take the average pma between the two years for ech month:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_grouped_pma\n",
    "monthly_grouped_pma = year_month_grouped_pma.groupby('month').years_monthly_pma.agg(average_monthly_pma='mean')\n",
    "# monthly_grouped_pma\n",
    "line_plot(monthly_grouped_pma, 'month', 'average_monthly_pma', 'Average Monthly pma', 'months', 'pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, It is clear enough now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us find the hours of the day with most power demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_grouped_pma = df.groupby('hour').pma.agg(hourly_pma='mean')\n",
    "#hourly_grouped_pma\n",
    "line_plot(hourly_grouped_pma, 'hour', 'hourly_pma', 'Average Hourly PMA During One Day', 'Hour', 'pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this one is for both two years, let us do it for each season and try to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "\n",
    "def map_season(month, day):\n",
    "    if (month == 12 and day >= 21) or (month == 1) or (month == 2) or (month == 3 and day < 21):\n",
    "        return seasons[0]  # Winter\n",
    "    elif (month == 3 and day >= 21) or (month == 4) or (month == 5) or (month == 6 and day < 21):\n",
    "        return seasons[1]  # Spring\n",
    "    elif (month == 6 and day >= 21) or (month == 7) or (month == 8) or (month == 9 and day < 21):\n",
    "        return seasons[2]  # Summer\n",
    "    else:\n",
    "        return seasons[3]  # Fall\n",
    "\n",
    "# Assign seasons to each day, month, and hour\n",
    "df['season'] = df.apply(lambda row: map_season(row.month, row.day), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasoned_hourly_grouped_pma = df.groupby(['season', 'hour']).pma.agg(seasoned_hourly_pma='mean')\n",
    "fall_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Fall']\n",
    "winter_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Winter']\n",
    "spring_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Spring']\n",
    "summer_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Summer']\n",
    "\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.title('Average Hourly PMA During Each Season', fontdict=title_style)\n",
    "plt.ylabel('seasoned pma', fontdict=labels_style)\n",
    "plt.xlabel('hour', fontdict=labels_style)\n",
    "\n",
    "for season in seasons:\n",
    "    seasoned_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')==season]\n",
    "    #line_plot(seasoned_hourly, 'hour', 'seasoned_hourly_pma', f'Average Hourly Demand in {season}', 'Hour', 'pma')\n",
    "    sns.lineplot(data=seasoned_hourly, x='hour', y='seasoned_hourly_pma', label=season)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Power demand reaches its maximum values during summer. Which logical, most people are in holidays thus staying at home most of the time compared to the rest of the year.\n",
    "* During one day, PMA reaches its climax at around 8pm.\n",
    "* During Winter, max demand is at its peak before 8pm, and after that time it starts decreasing. This could be due to many reasons. One of them is that people tend to sleep earlier at winter. Meanwhile during summer, it reaches its climax after 8pm and higher values as well 1pm and 4pm. Because, most people are at their homes with their AC (Air Conditioner) on at those times due to high temperatures outside.\n",
    "* Demand is low in all seasons during night (most logically) and medium during day where everyone are doing their activities and daily tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "def scatter_plot(data, x, y, title='', xlabel='', ylabel='', rotate_x=0):\n",
    "    title_style = {'family':'serif','color':'darkblue','size':18, 'weight':'bold'}\n",
    "    labels_style = {'family':'serif','color':'black','size':15}\n",
    "    plt.figure(figsize=[12, 6])\n",
    "    plt.title(title, fontdict=title_style)\n",
    "    plt.ylabel(ylabel, fontdict=labels_style)\n",
    "    plt.xlabel(xlabel, fontdict=labels_style)\n",
    "    plt.xticks(rotation=rotate_x)\n",
    "    sns.regplot(data=data, x=x, y=y)\n",
    "    plt.show()\n",
    "scatter_plot(df, 'tmp', 'pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us understand the study we are conducting and its aim. We are trying to identify day types of Algerian electricity load. Given the maximum power demand (pma) and termperature each hour in each day from January 1st, 2016 to December 31st, 2017, we will group our data and reshape it a bit to get each day as a data object with its pma (the average of the day) and temperature (the average in that day).  \n",
    "**Note:** be careful in working with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up another column in order to group by it\n",
    "df['day'] = df.time.dt.day\n",
    "df['month'] = df.time.dt.month\n",
    "df['year'] = df.time.dt.year\n",
    "df['fullday'] = pd.to_datetime(df[['year', 'month', 'day']]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the new dataframe\n",
    "df_daily = df.groupby('fullday').agg({'pma': 'mean', 'tmp': 'mean'}).reset_index()\n",
    "df_daily.set_index('fullday', inplace=True)\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before clustering, we will reduce the dimentionality of the data by applying PCA dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "df_daily_transformed = pca.fit_transform(df_daily)\n",
    "df_daily_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, strandardization of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_daily_transformed = scaler.fit_transform(df_daily_transformed)\n",
    "df_daily_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to find the best k number of clusters for K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Elbow Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster our transformed dataframe and plot the Sum of Squared Errors to get a Scree Plot. Then, based on it, we choose the best k number of clusters (elbow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_value = range(1, 11)\n",
    "\n",
    "for k in k_value:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(df_daily_transformed)\n",
    "    sse.append(kmeans.inertia_) # inertia is SSE    \n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(k_values, sse, 'bx-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Sum of Squared Errors')\n",
    "plt.title('Elbow Method To Find Optimal Number Of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the optimal number of clusters based on the Scree Plot is **k=5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Silhouette Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, The silhouette coefficient or silhouette score kmeans is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = []\n",
    "k_value = range(2, 12) # silhouette needs at least 2 clusters\n",
    "\n",
    "for k in k_value:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(df_daily_transformed)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    silhouette.append(silhouette_score(df_daily_transformed, cluster_labels))\n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(k_values, silhouette, 'bx-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette')\n",
    "plt.title('Silhouette Metric To Find Optimal Number Of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Silhouette metric, the optimal value for **k** is **2**\n",
    "thus, we choose **k=5** as the we deduced from the Elbow method. And we cluster based on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "cluster = kmeans.fit(df_daily_transformed)\n",
    "cluster_labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# plotting the clustering results\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.scatter(df_daily_transformed[:, 0], df_daily_transformed[:, 1], c=cluster_labels)\n",
    "plt.title(f'K-Means Clustering with k={k}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try Hierarchical clustering with DBSCAN, directly on our daily dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.plot.scatter(x='tmp', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, we ought to scale pma and tmp to be approximately on the same scale. For this, we will use **min-max scaling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "\n",
    "df_daily_scaled = df_daily.copy()\n",
    "df_daily_scaled[['pma', 'tmp']] = minmax.fit_transform(df_daily[['pma', 'tmp']])\n",
    "\n",
    "df_daily_scaled.plot.scatter(x='tmp', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we cluster using DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 20 # change\n",
    "min_samples = 6 # change\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "dbscan.fit(df_daily)\n",
    "labels = dbscan.labels_\n",
    "df_daily_scaled['cluster_id'] = labels\n",
    "df_daily_scaled.cluster_id.unique()\n",
    "df_daily_scaled.plot.scatter(x='tmp', y='pma', c=labels, cmap='viridis') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we deduce having five types of days... **<!elaboration!>** {very high demand day, high demand day, seasonal demand day, low demand day, very low demand day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "dm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
