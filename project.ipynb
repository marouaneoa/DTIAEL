{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Type Identification of Algerian Electricity Load\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "This Jupyter notebook explores the identification of different day types based on electricity load patterns in an Algerian city. We will analyze a dataset containing hourly recordings of Maximum Power Demand (PMA) and Temperature for two years, from January 1st, 2016, to December 31st, 2017.\n",
    "\n",
    "**Data Source:**\n",
    "\n",
    "The dataset used in this analysis is stored in a file named `pma.xlsx`. It contains three columns:\n",
    "\n",
    "* `time`: Date and time (hourly)\n",
    "* `pma`: Maximum Power Demand (MW)\n",
    "* `tmp`: Temperature (Â°C)\n",
    "\n",
    "**Software and Tools:**\n",
    "\n",
    "This project will utilize Python libraries such as:\n",
    "\n",
    "* `pandas` for data manipulation and analysis\n",
    "* `numpy` for scientific computing\n",
    "* `matplotlib` and `seaborn` for data visualization\n",
    "* `scikit-learn` for machine learning and clustering algorithms\n",
    "\n",
    "**Let's begin by importing the necessary libraries and reading the data into a Pandas dataframe.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pma</th>\n",
       "      <th>tmp</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>982.529002</td>\n",
       "      <td>6.405644</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>983.240592</td>\n",
       "      <td>5.932445</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>1002.780354</td>\n",
       "      <td>5.503807</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>1011.657004</td>\n",
       "      <td>5.112056</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 05:00:00</td>\n",
       "      <td>999.137230</td>\n",
       "      <td>4.751342</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time          pma       tmp        date  hour     day\n",
       "0 2016-01-01 01:00:00   982.529002  6.405644  2016-01-01     1  Friday\n",
       "1 2016-01-01 02:00:00   983.240592  5.932445  2016-01-01     2  Friday\n",
       "2 2016-01-01 03:00:00  1002.780354  5.503807  2016-01-01     3  Friday\n",
       "3 2016-01-01 04:00:00  1011.657004  5.112056  2016-01-01     4  Friday\n",
       "4 2016-01-01 05:00:00   999.137230  4.751342  2016-01-01     5  Friday"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_excel(\"pma.xlsx\", skiprows=1)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = [\"time\", \"pma\", \"tmp\"]\n",
    "\n",
    "# Convert time column to datetime\n",
    "df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "# Create a column for date only without time\n",
    "df['date'] = df['time'].dt.date\n",
    "# Create a column for time only without date\n",
    "df['hour'] = df['time'].dt.time\n",
    "# Create a column for day of the week\n",
    "df['day'] = df['time'].dt.day_name()\n",
    "# Create a column for hour of the day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "# Drop time column\n",
    "df.drop('time', axis=1)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1353\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1353\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1402\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;124;03mApply function f in python space\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m    data after applying f\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1402\u001b[0m values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39mapply(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/groupby/ops.py:767\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    766\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 767\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m, in \u001b[0;36mweekly_aggregation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweekly_aggregation\u001b[39m(df):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW-Sun\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/frame.py:10994\u001b[0m, in \u001b[0;36mDataFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, on, level, origin, offset, group_keys)\u001b[0m\n\u001b[1;32m  10979\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39mresample, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_shared_doc_kwargs)\n\u001b[1;32m  10980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresample\u001b[39m(\n\u001b[1;32m  10981\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10992\u001b[0m     group_keys: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m  10993\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Resampler:\n\u001b[0;32m> 10994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mresample(\n\u001b[1;32m  10995\u001b[0m         rule\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m  10996\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m  10997\u001b[0m         closed\u001b[38;5;241m=\u001b[39mclosed,\n\u001b[1;32m  10998\u001b[0m         label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m  10999\u001b[0m         convention\u001b[38;5;241m=\u001b[39mconvention,\n\u001b[1;32m  11000\u001b[0m         kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[1;32m  11001\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m  11002\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m  11003\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m  11004\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m  11005\u001b[0m         group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m  11006\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/generic.py:8888\u001b[0m, in \u001b[0;36mNDFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, on, level, origin, offset, group_keys)\u001b[0m\n\u001b[1;32m   8887\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_resampler(\n\u001b[1;32m   8889\u001b[0m     cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries | DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   8890\u001b[0m     freq\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m   8891\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m   8892\u001b[0m     closed\u001b[38;5;241m=\u001b[39mclosed,\n\u001b[1;32m   8893\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   8894\u001b[0m     kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[1;32m   8895\u001b[0m     convention\u001b[38;5;241m=\u001b[39mconvention,\n\u001b[1;32m   8896\u001b[0m     key\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m   8897\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   8898\u001b[0m     origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   8899\u001b[0m     offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m   8900\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m   8901\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/resample.py:1523\u001b[0m, in \u001b[0;36mget_resampler\u001b[0;34m(obj, kind, **kwds)\u001b[0m\n\u001b[1;32m   1522\u001b[0m tg \u001b[38;5;241m=\u001b[39m TimeGrouper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 1523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tg\u001b[38;5;241m.\u001b[39m_get_resampler(obj, kind\u001b[38;5;241m=\u001b[39mkind)\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/resample.py:1713\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[0;34m(self, obj, kind)\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TimedeltaIndexResampler(\n\u001b[1;32m   1706\u001b[0m         obj,\n\u001b[1;32m   1707\u001b[0m         timegrouper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1710\u001b[0m         gpr_index\u001b[38;5;241m=\u001b[39max,\n\u001b[1;32m   1711\u001b[0m     )\n\u001b[0;32m-> 1713\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly valid with DatetimeIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimedeltaIndex or PeriodIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ax)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1717\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW-Sun\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Group data by week and calculate weekly means\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_weekly \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(weekly_aggregation)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plot time series of weekly PMA and Temperature\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1363\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1355\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[0;32m-> 1363\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1402\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1402\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39mapply(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1404\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/groupby/ops.py:767\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[1;32m    766\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 767\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    769\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m, in \u001b[0;36mweekly_aggregation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweekly_aggregation\u001b[39m(df):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW-Sun\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/frame.py:10994\u001b[0m, in \u001b[0;36mDataFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, on, level, origin, offset, group_keys)\u001b[0m\n\u001b[1;32m  10979\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39mresample, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_shared_doc_kwargs)\n\u001b[1;32m  10980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresample\u001b[39m(\n\u001b[1;32m  10981\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10992\u001b[0m     group_keys: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m  10993\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Resampler:\n\u001b[0;32m> 10994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mresample(\n\u001b[1;32m  10995\u001b[0m         rule\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m  10996\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m  10997\u001b[0m         closed\u001b[38;5;241m=\u001b[39mclosed,\n\u001b[1;32m  10998\u001b[0m         label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m  10999\u001b[0m         convention\u001b[38;5;241m=\u001b[39mconvention,\n\u001b[1;32m  11000\u001b[0m         kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[1;32m  11001\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m  11002\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m  11003\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m  11004\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m  11005\u001b[0m         group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m  11006\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/generic.py:8888\u001b[0m, in \u001b[0;36mNDFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, on, level, origin, offset, group_keys)\u001b[0m\n\u001b[1;32m   8885\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_resampler\n\u001b[1;32m   8887\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_resampler(\n\u001b[1;32m   8889\u001b[0m     cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries | DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   8890\u001b[0m     freq\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m   8891\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m   8892\u001b[0m     closed\u001b[38;5;241m=\u001b[39mclosed,\n\u001b[1;32m   8893\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   8894\u001b[0m     kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[1;32m   8895\u001b[0m     convention\u001b[38;5;241m=\u001b[39mconvention,\n\u001b[1;32m   8896\u001b[0m     key\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m   8897\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   8898\u001b[0m     origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   8899\u001b[0m     offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m   8900\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m   8901\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/resample.py:1523\u001b[0m, in \u001b[0;36mget_resampler\u001b[0;34m(obj, kind, **kwds)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;124;03mCreate a TimeGrouper and return our resampler.\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1522\u001b[0m tg \u001b[38;5;241m=\u001b[39m TimeGrouper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 1523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tg\u001b[38;5;241m.\u001b[39m_get_resampler(obj, kind\u001b[38;5;241m=\u001b[39mkind)\n",
      "File \u001b[0;32m~/anaconda3/envs/DM_ENV/lib/python3.11/site-packages/pandas/core/resample.py:1713\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[0;34m(self, obj, kind)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, TimedeltaIndex):\n\u001b[1;32m   1705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TimedeltaIndexResampler(\n\u001b[1;32m   1706\u001b[0m         obj,\n\u001b[1;32m   1707\u001b[0m         timegrouper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1710\u001b[0m         gpr_index\u001b[38;5;241m=\u001b[39max,\n\u001b[1;32m   1711\u001b[0m     )\n\u001b[0;32m-> 1713\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly valid with DatetimeIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimedeltaIndex or PeriodIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ax)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1717\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'"
     ]
    }
   ],
   "source": [
    "def weekly_aggregation(df):\n",
    "    return df.resample(\"W-Sun\").mean()\n",
    "\n",
    "# Group data by week and calculate weekly means\n",
    "df_weekly = df.groupby(\"date\").apply(weekly_aggregation)\n",
    "\n",
    "# Plot time series of weekly PMA and Temperature\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_weekly.index, df_weekly[\"pma\"], label=\"PMA\")\n",
    "plt.plot(df_weekly.index, df_weekly[\"tmp\"], label=\"Temperature\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Weekly Time Series of PMA and Temperature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of rows: {df.shape[0]}')\n",
    "print(f'number of columns: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_values = df.isna().sum().sum()\n",
    "print(f'total of missing values: {total_missing_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duplicates = df.duplicated().sum()\n",
    "print(f'total of missing values: {total_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spread tme into several columns for better groupping\n",
    "df['hour'] = df.time.dt.hour\n",
    "df['day'] = df.time.dt.day\n",
    "df['month'] = df.time.dt.month\n",
    "df['year'] = df.time.dt.year\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over all time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try to see the development of the maximum power demand over time in 2016 and 2017 by months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group\n",
    "year_month_grouped_pma = df.groupby(['year', 'month']).pma.agg(years_monthly_pma='mean')\n",
    "\n",
    "title_style = {'family':'serif','color':'darkblue','size':18, 'weight':'bold'}\n",
    "labels_style = {'family':'serif','color':'black','size':15}\n",
    "def line_plot(data, x, y, title='', xlabel='', ylabel='', rotate_x=0):\n",
    "    plt.figure(figsize=[12, 6])\n",
    "    plt.title(title, fontdict=title_style)\n",
    "    plt.ylabel(ylabel, fontdict=labels_style)\n",
    "    plt.xlabel(xlabel, fontdict=labels_style)\n",
    "    plt.xticks(rotation=rotate_x)\n",
    "    sns.lineplot(data=data, x=x, y=y)\n",
    "    plt.show()\n",
    "\n",
    "# adding a column for the combination year-month\n",
    "year_month_grouped_pma['year_month'] = year_month_grouped_pma.index.map(lambda x: f'{x[0]}' + '-' + f'{x[1]}')\n",
    "# year_month_grouped_pma\n",
    "\n",
    "# plotting the data\n",
    "line_plot(year_month_grouped_pma, 'year_month', 'years_monthly_pma', 'monthly pma between 2016 and 2017', 'year-month', 'pma', 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice, for now, maximum power demand increases mostly during summer from May till October.  \n",
    "To see this better, let us take the average pma between the two years for ech month:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_grouped_pma\n",
    "monthly_grouped_pma = year_month_grouped_pma.groupby('month').years_monthly_pma.agg(average_monthly_pma='mean')\n",
    "# monthly_grouped_pma\n",
    "line_plot(monthly_grouped_pma, 'month', 'average_monthly_pma', 'Average Monthly pma', 'months', 'pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, It is clear enough now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMA over days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us find the hours of the day with most power demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_grouped_pma = df.groupby('hour').pma.agg(hourly_pma='mean')\n",
    "#hourly_grouped_pma\n",
    "line_plot(hourly_grouped_pma, 'hour', 'hourly_pma', 'Average Hourly PMA During One Day', 'Hour', 'pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this one is for both two years, let us do it for each season and try to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "\n",
    "def map_season(month, day):\n",
    "    if (month == 12 and day >= 21) or (month == 1) or (month == 2) or (month == 3 and day < 21):\n",
    "        return seasons[0]  # Winter\n",
    "    elif (month == 3 and day >= 21) or (month == 4) or (month == 5) or (month == 6 and day < 21):\n",
    "        return seasons[1]  # Spring\n",
    "    elif (month == 6 and day >= 21) or (month == 7) or (month == 8) or (month == 9 and day < 21):\n",
    "        return seasons[2]  # Summer\n",
    "    else:\n",
    "        return seasons[3]  # Fall\n",
    "\n",
    "# Assign seasons to each day, month, and hour\n",
    "df['season'] = df.apply(lambda row: map_season(row.month, row.day), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasoned_hourly_grouped_pma = df.groupby(['season', 'hour']).pma.agg(seasoned_hourly_pma='mean')\n",
    "fall_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Fall']\n",
    "winter_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Winter']\n",
    "spring_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Spring']\n",
    "summer_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')=='Summer']\n",
    "\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.title('Average Hourly PMA During Each Season', fontdict=title_style)\n",
    "plt.ylabel('seasoned pma', fontdict=labels_style)\n",
    "plt.xlabel('hour', fontdict=labels_style)\n",
    "\n",
    "for season in seasons:\n",
    "    seasoned_hourly = seasoned_hourly_grouped_pma[seasoned_hourly_grouped_pma.index.get_level_values('season')==season]\n",
    "    #line_plot(seasoned_hourly, 'hour', 'seasoned_hourly_pma', f'Average Hourly Demand in {season}', 'Hour', 'pma')\n",
    "    sns.lineplot(data=seasoned_hourly, x='hour', y='seasoned_hourly_pma', label=season)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Power demand reaches its maximum values during summer. Which logical, most people are in holidays thus staying at home most of the time compared to the rest of the year.\n",
    "* During one day, PMA reaches its climax at around 8pm.\n",
    "* During Winter, max demand is at its peak before 8pm, and after that time it starts decreasing. This could be due to many reasons. One of them is that people tend to sleep earlier at winter. Meanwhile during summer, it reaches its climax after 8pm and higher values as well 1pm and 4pm. Because, most people are at their homes with their AC (Air Conditioner) on at those times due to high temperatures outside.\n",
    "* Demand is low in all seasons during night (most logically) and medium during day where everyone are doing their activities and daily tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "def scatter_plot(data, x, y, title='', xlabel='', ylabel='', rotate_x=0):\n",
    "    title_style = {'family':'serif','color':'darkblue','size':18, 'weight':'bold'}\n",
    "    labels_style = {'family':'serif','color':'black','size':15}\n",
    "    plt.figure(figsize=[12, 6])\n",
    "    plt.title(title, fontdict=title_style)\n",
    "    plt.ylabel(ylabel, fontdict=labels_style)\n",
    "    plt.xlabel(xlabel, fontdict=labels_style)\n",
    "    plt.xticks(rotation=rotate_x)\n",
    "    sns.regplot(data=data, x=x, y=y)\n",
    "    plt.show()\n",
    "scatter_plot(df, 'tmp', 'pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us understand the study we are conducting and its aim. We are trying to identify day types of Algerian electricity load. Given the maximum power demand (pma) and termperature each hour in each day from January 1st, 2016 to December 31st, 2017, we will group our data and reshape it a bit to get each day as a data object with its pma (the average of the day) and temperature (the average in that day).  \n",
    "**Note:** be careful in working with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up another column in order to group by it\n",
    "df['day'] = df.time.dt.day\n",
    "df['month'] = df.time.dt.month\n",
    "df['year'] = df.time.dt.year\n",
    "df['fullday'] = pd.to_datetime(df[['year', 'month', 'day']]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the new dataframe\n",
    "df_daily = df.groupby('fullday').agg({'pma': 'mean', 'tmp': 'mean'}).reset_index()\n",
    "df_daily.set_index('fullday', inplace=True)\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before clustering, we will reduce the dimentionality of the data by applying PCA dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "df_daily_transformed = pca.fit_transform(df_daily)\n",
    "df_daily_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, strandardization of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_daily_transformed = scaler.fit_transform(df_daily_transformed)\n",
    "df_daily_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to find the best k number of clusters for K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Elbow Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster our transformed dataframe and plot the Sum of Squared Errors to get a Scree Plot. Then, based on it, we choose the best k number of clusters (elbow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_value = range(1, 11)\n",
    "\n",
    "for k in k_value:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(df_daily_transformed)\n",
    "    sse.append(kmeans.inertia_) # inertia is SSE    \n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(k_values, sse, 'bx-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Sum of Squared Errors')\n",
    "plt.title('Elbow Method To Find Optimal Number Of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the optimal number of clusters based on the Scree Plot is **k=5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Silhouette Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, The silhouette coefficient or silhouette score kmeans is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = []\n",
    "k_value = range(2, 12) # silhouette needs at least 2 clusters\n",
    "\n",
    "for k in k_value:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(df_daily_transformed)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    silhouette.append(silhouette_score(df_daily_transformed, cluster_labels))\n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(k_values, silhouette, 'bx-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette')\n",
    "plt.title('Silhouette Metric To Find Optimal Number Of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Silhouette metric, the optimal value for **k** is **2**\n",
    "thus, we choose **k=5** as the we deduced from the Elbow method. And we cluster based on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "cluster = kmeans.fit(df_daily_transformed)\n",
    "cluster_labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# plotting the clustering results\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.scatter(df_daily_transformed[:, 0], df_daily_transformed[:, 1], c=cluster_labels)\n",
    "plt.title(f'K-Means Clustering with k={k}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try Hierarchical clustering with DBSCAN, directly on our daily dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.plot.scatter(x='tmp', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, we ought to scale pma and tmp to be approximately on the same scale. For this, we will use **min-max scaling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "\n",
    "df_daily_scaled = df_daily.copy()\n",
    "df_daily_scaled[['pma', 'tmp']] = minmax.fit_transform(df_daily[['pma', 'tmp']])\n",
    "\n",
    "df_daily_scaled.plot.scatter(x='tmp', y='pma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we cluster using DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 20 # change\n",
    "min_samples = 6 # change\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "dbscan.fit(df_daily)\n",
    "labels = dbscan.labels_\n",
    "df_daily_scaled['cluster_id'] = labels\n",
    "df_daily_scaled.cluster_id.unique()\n",
    "df_daily_scaled.plot.scatter(x='tmp', y='pma', c=labels, cmap='viridis') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we deduce having five types of days... **<!elaboration!>**  \n",
    "{very high demand day, high demand day, seasonal demand day, low demand day, very low demand day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
